import sys
import os
import json
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline

# Permet d'importer les modules src.* m√™me si on lance le script depuis un sous-dossier
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

# 1. Chargement du mod√®le d'embeddings (pour la recherche s√©mantique)
print("Chargement du mod√®le s√©mantique...")
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# 2. Chargement de la FAQ depuis le fichier JSON
chemin_faq = os.path.join(os.path.dirname(__file__), "../../data/raw/faq_base.json")
with open(chemin_faq, 'r', encoding='utf-8') as f:
    data = json.load(f)

# On garde les r√©ponses compl√®tes (meilleur contexte pour l'extraction)
reponses_faq = [item['answer'] for item in data['faq']]
print(f"üìö {len(reponses_faq)} r√©ponses charg√©es depuis la FAQ")

# 3. Pour la recherche s√©mantique, on combine question + r√©ponse
documents_recherche = [
    f"Question: {item['question']} R√©ponse: {item['answer']}" 
    for item in data['faq']
]
corpus_embeddings = embedder.encode(documents_recherche, convert_to_tensor=True)

# 4. Chargement du mod√®le extractif (CamemBERT pour le fran√ßais)
print("Chargement du mod√®le extractif CamemBERT...")
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_name = "etalab-ia/camembert-base-squadFR-fquad-piaf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

extracteur = pipeline(
    "question-answering",
    model=model,
    tokenizer=tokenizer,
    device=-1
)
print("‚úÖ Mod√®le extractif pr√™t")


def interroger_extractif(question):
    """
    Strat√©gie C : Q&A Extractif
    1. Recherche s√©mantique du document pertinent
    2. Extraction de la r√©ponse exacte avec Roberta
    """
    print(f"\n--- Strat√©gie C : Q&A Extractif ---")
    print(f"Question : {question}")
    
    # A. Recherche du document le plus pertinent (comme Strat√©gie B)
    query_embedding = embedder.encode(question, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=1)
    
    meilleur_hit = hits[0][0]
    index_doc = meilleur_hit['corpus_id']
    score = meilleur_hit['score']
    
    # On utilise la R√âPONSE compl√®te comme contexte (meilleur pour l'extraction)
    contexte = reponses_faq[index_doc]
    
    print(f"Document trouv√© (Pertinence: {score:.4f})")
    print(f"Contexte : {contexte[:100]}...")
    
    # B. Extraction de la r√©ponse exacte avec Roberta
    try:
        resultat = extracteur(question=question, context=contexte)
        
        reponse_extraite = resultat['answer']
        confiance = resultat['score']
        
        print(f"\n‚úÖ R√©ponse extraite (Confiance: {confiance:.2%}) :")
        print(f"   {reponse_extraite}")
        
        # Validation du score de confiance
        return valider_reponse(reponse_extraite, confiance)
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'extraction : {e}")
        return "Je n'ai pas pu extraire une r√©ponse pr√©cise du document."

def valider_reponse(reponse: str, score: float, seuil: float = 0.20) -> str:
    """
    V√©rifie si la r√©ponse extraite est assez fiable.
    Si le score est trop bas, on pr√©f√®re dire qu'on ne sait pas.
    """
    if score < seuil:
        return f"Je ne suis pas assez s√ªr de la r√©ponse (Confiance: {score:.1%}). Pouvez-vous reformuler ?"
    return reponse


if __name__ == "__main__":
    # Test avec une question
    q = "Comment immatriculer une voiture ?"
    reponse = interroger_extractif(q)
    
    print("\n" + "="*60)
    print("R√âPONSE FINALE :")
    print(reponse)  