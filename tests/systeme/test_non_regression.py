import sys
import os
import pytest

# Ajout du root du projet au sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from src.benchmark.run_benchmark import charger_golden_set, calculer_score_keywords
from src.strategies.strategie_b_rag import interroger_rag

def test_performance_rag_golden_set():
    """
    Test de non-r√©gression : V√©rifie que la Strat√©gie B (RAG) 
    maintient un niveau de performance acceptable sur le Golden Set.
    """
    print("\nüöÄ Lancement du test de non-r√©gression (Strat√©gie B)...")
    
    # 1. Charger le Golden Set
    golden_set = charger_golden_set()
    
    # On se concentre sur les questions de type 'direct_match' pour le test de CI
    # pour √©viter de tout faire tourner (gain de temps)
    questions_test = [q for q in golden_set if q['type'] == 'direct_match']
    
    scores = []
    
    # 2. Ex√©cuter la strat√©gie sur ce sous-ensemble
    for item in questions_test:
        question = item['question']
        expected_keywords = item.get('expected_keywords', [])
        
        try:
            reponse = interroger_rag(question)
            score = calculer_score_keywords(reponse, expected_keywords)
            scores.append(score)
            print(f"   ‚úÖ Question: {item['id']} | Score: {score}%")
        except Exception as e:
            print(f"   ‚ùå Question: {item['id']} | Erreur: {e}")
            scores.append(0)

    # 3. Calculer la moyenne
    score_moyen = sum(scores) / len(scores) if scores else 0
    print(f"\nüìä Score moyen sur le Golden Set (Direct Match) : {score_moyen:.1f}%")

    # 4. ASSERT : On exige au moins 55% de r√©ussite sur les correspondances directes
    # (C'est notre seuil de non-r√©gression)
    assert score_moyen >= 55, f"R√©gression d√©tect√©e ! Le score moyen ({score_moyen}%) est inf√©rieur au seuil de 55%."
