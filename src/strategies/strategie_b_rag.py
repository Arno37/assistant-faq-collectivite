import sys
import os
import torch
from sentence_transformers import SentenceTransformer, util
import json 

# Permet d'importer les modules src.* m√™me si on lance le script depuis un sous-dossier
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
from src.outils.client_ia import obtenir_client_hf, MODELE_LLM

# 1. Chargement du mod√®le d'embeddings (le "traducteur" texte -> nombres)
print("Chargement du mod√®le s√©mantique...")
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# 2. Chargement de la vraie FAQ depuis le fichier JSON
chemin_faq = os.path.join(os.path.dirname(__file__), "../../data/raw/faq_base.json")
with open(chemin_faq, 'r', encoding='utf-8') as f:
    data = json.load(f)

# On extrait les r√©ponses (c'est ce qu'on va chercher s√©mantiquement)
documents_faq = [
    f"Question: {item['question']} R√©ponse: {item['answer']}" 
    for item in data['faq']
]
print(f"üìö {len(documents_faq)} documents charg√©s depuis la FAQ")

# 3. On transforme les phrases en nombres (Vecteurs) une bonne fois pour toutes
corpus_embeddings = embedder.encode(documents_faq, convert_to_tensor=True)

client = obtenir_client_hf()

def construire_prompt_rag(question, contexte):
    """
    Construit le prompt syst√®me pour le mod√®le RAG.
    Prend en entr√©e la question et une liste de documents (contexte).
    """
    # On combine les documents en un seul texte
    contexte_combine = "\n\n".join(contexte)
    
    return f"""
Tu es un assistant de mairie. Utilise EXCLUSIVEMENT le contexte ci-dessous pour r√©pondre.
Si la r√©ponse n'est pas dans le contexte, dis "Je ne sais pas".

CONTEXTE : 
{contexte_combine}
"""

def interroger_rag(question):
    print(f"\nRecherche pour : {question}")
    
    # A. Recherche des 3 documents les plus pertinents
    query_embedding = embedder.encode(question, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)
    
    # R√©cup√©rer les 3 meilleurs documents
    top_docs = [documents_faq[hit['corpus_id']] for hit in hits[0]]
    scores = [hit['score'] for hit in hits[0]]
    
    print(f"\nüìä Top 3 documents trouv√©s :")
    for i, (doc, score) in enumerate(zip(top_docs, scores), 1):
        print(f"  {i}. Pertinence: {score:.4f} - {doc[:80]}...")
    
    # B. Construction de la r√©ponse avec l'IA
    prompt_systeme = construire_prompt_rag(question, top_docs)
    
    messages = [
        {"role": "system", "content": prompt_systeme},
        {"role": "user", "content": question}
    ]
    
    reponse = client.chat_completion(
        model=MODELE_LLM,
        messages=messages, 
        max_tokens=150
    )
    return reponse.choices[0].message.content

if __name__ == "__main__":
    q = "Je veux r√©server la salle municipale, comment faire ?"
    print("R√©ponse IA :", interroger_rag(q))